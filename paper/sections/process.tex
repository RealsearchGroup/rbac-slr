\section{Methodology and Process} \label{sec:process}

We adopted and applied a systematic literature review process following recommendations from Kitchenham's suggested processes~\cite{kitchenham2007guidelines}.  The systematic literature review process was broken down into four stages and the rest of this section is broken down by each stage.  The stages were as follows:

\begin{itemize}
\setlength{\itemsep}{0.25pt}
\item Step 1: Development of a search strategy
\item Step 2: Elimination of papers based on title criteria
\item Step 3: Elimination of papers based on abstract criteria
\item Step 4: Elimination of papers based on content and elimination criteria
\end{itemize}

\subsection{Step 1: Search Strategy}

For the first phase of our systematic literature review, we developed a search strategy for finding papers.  The search strategy was executed by an automated comprehensive search taking as input a set of academic search engines and a list of search terms.  The search was performed by applying each search term to each engine incrementally until the stopping criteria were met. Table \ref{tab:search_results} lists the four search engines along the left most column with the three search terms across the top row along with the papers for each criterion and engine combination.  The search algorithm was performed as follows:

\begin{enumerate}
\setlength{\itemsep}{0.25pt}
\item Call to search engine with current search position and current search term.
\item Parse results and extract paper title, authors and year of publication.
\item Compare results against stopping criteria:
	\begin{itemize}
	\item If the size of the result set is greater than or equal to 1000 then stop.
	\item If the last ten results did not contain the search term phrase within the title then stop.
	\end{itemize}
\item If stopping criteria not met, increment search position and go back to step one.
\end{enumerate}

The result set size stopping criteria was chosen due to a technical limitation of some search engines.  The stopping criteria related to the last ten titles are meant to stop after relevant results are no longer being returned by the search engine.  After gathering all 12 data sets, we combined the papers into a master list, which includes only distinct papers by systematically comparing the bibliographic information for each.
Out of the master list of 1,716 papers, two reviewers, denoted by Reviewer 1 and Reviewer 2, conducted a series of elimination rounds to narrow the list of papers and identify primary sources. Table \ref{tab:eliminations} shows the total number of papers selected by each reviewer for each round and how many papers from the disjoint set for each round survived to the next round.

\begin{table}
\centering
\caption{Paper counts after applying search strategy}
\vspace{0.1 in}
\begin{tabular}{ | l | r | r | r | r | }

\hline
 & 
\textbf{RBAC} & 
\textbf{role based access control} & 
\textbf{role-based access control} & 
\textbf{Total}
\\\hline

Google Scholar & 651 & 213 & 435 & 1299 \\\hline
ACM Portal & 500 & 20 & 720 & 1240 \\\hline
IEEExplore & 200 & 40 & 230 & 470 \\\hline
CiteSeerX & 100 & 100 & 150 & 350 \\\hline
 &  &  &  & \\\hline
Totals & 1451 & 373 & 1535 & 3359 \\\hline
Combined &  &  &  & \textbf{1716} \\\hline

\end{tabular}
\label{tab:search_results}
\end{table}

\subsection{Steps 2-4: Elimination Rounds}

The elimination rounds were conducted based on reading of the title, abstract, and finally the paper itself.  While each elimination stage had a unique set of criteria for elimination, the general procedure for elimination for the researchers was as follows.

\begin{itemize}
\setlength{\itemsep}{0.25pt}
\item The two first authors independently classified papers as relevant, irrelevant or uncertain based on elimination criteria
\item Those papers marked as relevant by both reviewers were kept and those marked irrelevant by both were thrown out.
\item Papers marked as relevant or irrelevant by a single reviewer were combined with all papers marked as uncertain and discussed by both reviewers.  From this discussion, papers were either thrown out or kept until the next round of the review.
\end{itemize}

\subsubsection{Step 2: Title Elimination}

The first round of elimination was performed by examination based on the title.  Each author was tasked with deciding on elimination by answering the following questions:

\begin{itemize}
\setlength{\itemsep}{0.25pt}
\item Did the title contain a reference to 'role-based access control' or 'RBAC'? 
\item Did the title contain a reference to 'model'?
\end{itemize}

The title elimination round resulted in Reviewer 1 selecting 305 papers, and Reviewer 2 selecting 176 papers with 149 papers of overlap between the two. 
There were 332 papers found not to be in common, of which, 149 were rejected and 141 retained after a second review.

\subsubsection{Step 3: Abstract Elimination}

The second round of elimination was based on reading of the abstracts of papers that survived title elimination.  Researchers read each abstract and evaluated relevancy based off:

\begin{itemize}
\setlength{\itemsep}{0.25pt}
\item Does the abstract mention a proposed model?
\item Does the abstract mention extension of role-based access control?
\item Does the abstract mention an implementation, evaluation, or domain for their model?
\end{itemize}

The abstract elimination round resulted in Reviewer 1 selecting 86 papers, and Reviewer 2 selecting 102 papers with 51 papers of overlap between the two. 
There were 137 papers found not to be in common, of which, 116 were rejected and 21 retained after a second review.

\subsubsection{Step 4: Content Elimination}

The final elimination round involved reading the entire paper and answering five questions that would serve as the basis for elimination.  The data collected by answering these questions served as the basis towards answering the research questions.  Each reviewer seeks to answer following questions based on the content of the paper:

\begin{enumerate}
\setlength{\itemsep}{0.25pt}
\item Does this model extend the RBAC Reference Model (Exclusion)
\item Do the researchers give evidence that the RBAC Reference Model needs extension? (Inclusion)
\item Was the paper and subsequent model inspired by a real world example?  (Conditional Inclusion)
\item Did the researchers offer any evaluation of the proposed model? If yes, how did they do one? If no, why? (Conditional Inclusion)
\item Did the authors implement their model? (Inclusion)
\end{enumerate}

Question 1 was a definitive exclusion criterion as any paper that failed in the affirmative was rejected.  Questions 3 and 4 were marked as conditional includes given that they were connected in making a decision. 
A paper that met question 3 but not 4, or met 4 but not 3 would be included because for some cases the real world examples served as research evaluations and without this conditional include the paper list size would be too small to be significant.

The content elimination round resulted in Reviewer 1 selecting 46 papers, and Reviewer 2 selecting 42 papers with 24 papers of overlap between the two. 
Between the two reviewers selections, there were 64 papers not in common, of which, 59 were rejected and 5 retained after a second review.

\begin{table}
\centering
\caption{Elimination Rounds}
\vspace{0.1 in}
\begin{tabular}{ l l | r | r | r | }
\cline{3-5}

 & \multicolumn{1}{ c| }{} & \textbf{Title} & \textbf{Abstract} & \textbf{Content} \\ \hline

\multicolumn{1}{ |c| }{Reviewer 1}  &  & 305 & 86 & 46 \\ \hline
\multicolumn{1}{ |c| }{Reviewer 2} &  & 176 & 102 & 42 \\ \hline

\multicolumn{1}{ c| }{} & Overlap & 149 & 51 & 24 \\ \cline{2-5}
\multicolumn{1}{ c| }{} & Disjoint & 332 & 137 & 64 \\ \cline{2-5}
\multicolumn{1}{ c| }{} & Rejected & 191 & 116 & 61 \\ \cline{2-5}
\multicolumn{1}{ c| }{} & Retained & 141 & 21 & 5 \\ \cline{2-5}
\multicolumn{1}{ c| }{} & \textbf{Num Left} & 290 & 72 & 27 \\ \cline{2-5}

\end{tabular}
\label{tab:eliminations}
\end{table}


\subsection{Extraction}

After selection of primary sources, the next step was to extract data from each paper that pertained to our research questions in order to look for trends. 
The first step was to take the individual data generated from the final elimination round and organize this information around the research questions. 
During the paper reading round and resulting data, the fact that the papers were falling into a number of categorizations became evident. 
Thus, the first step undertaken was to answer the question of what categories exist for the RBAC extension models and what papers fell into what categories.
