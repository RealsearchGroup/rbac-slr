\section{Methodology and Process} \label{sec:process}

The authors adopted and applied a systematic literature review following recommendations from Kitchenham's suggested processes \cite{kitchenham2007guidelines}.  The systematic literature review process was broken down in to four stages and the rest of this section is broken down by each stage.  The stages were:

\begin{itemize}
\setlength{\itemsep}{0.25pt}
\item Step 1: Development of a search strategy
\item Step 2: Elimination of papers based on title criteria
\item Step 3: Elimination of papers based on abstract criteria
\item Step 4: Elimination of papers based on content and matching to elimination criteria
\end{itemize}

\subsection{Step 1: Search Strategy}

For the first phase of our systematic literature review, the authors developed a search strategy for finding papers.  The search strategy was executed by an automated comprehensive search taking as input a set of academic search engines and a list of search terms.  The search was performed by applying each search term to each engine incrementally until a stopping criteria was met. Table \ref{tab:search_results} lists the four search engines along the left most column with the three search term across the top row along with the papers for each criteria and engine combination.  The search algorithm was performed as follows:

\begin{enumerate}
\setlength{\itemsep}{0.25pt}
\item Call to search engine with current search position and current search term.
\item Parse results and extract paper title, authors and year of publication.
\item Compare results against stopping criteria:
	\begin{itemize}
	\item If the size of the result set is greater than or equal to 1000 then stop.
	\item If the last ten results did not contain the search term phrase within the tite then stop.
	\end{itemize}
\item If stopping criteria not met, increment search position and go back to step one.
\end{enumerate}

The result set size stopping criteria was chosen due to a technical limitation of some search engines.  The stopping criteria related to the last ten titles is meant to stop after relevant results are no longer being returned by the search engine.  After gathering all 12 data sets, the authors combined the papers into a master list by systematically comparing the bibliographic information for each.  The master list of 1716 papers was used to perform a series of elimination rounds to narrow the list of papers and identify primary sources.  Table \ref{tab:eliminations} shows the total number of papers selected by each author for each round and how many papers from the disjoint set for each round survived to the next round.

\begin{table}
\centering
\begin{tabular}{|p{3.5cm}|p{1.25cm}|p{4.25cm}|p{4.25cm}|p{1cm}|}

\hline
 & 
\textbf{RBAC} & 
\textbf{role based access control} & 
\textbf{role-based access control} & 
\textbf{Total}
\\\hline

Google Scholar & 651 & 213 & 435 & 1299 \\\hline
ACM Portal & 500 & 20 & 720 & 1240 \\\hline
IEEExplore & 200 & 40 & 230 & 470 \\\hline
CiteSeerX & 100 & 100 & 150 & 350 \\\hline
 &  &  &  & \\\hline
Totals & 1451 & 373 & 1535 & 3359 \\\hline
Combined &  &  &  & \textbf{1716} \\\hline

\end{tabular}
\caption{Paper counts after applying search strategy}
\label{tab:search_results}
\end{table}

\subsection{Steps 2-4: Elimination Rounds}

The elimination rounds were performed based on reading of the title, abstract and finally the paper itself.  While each elimination stage had a unique set of criteria for elimination the general procedure for elimination for the researchers was as follows.

\begin{itemize}
\setlength{\itemsep}{0.25pt}
\item The two first authors independently classified papers as relevant, irrelevant or uncertain based off elimination criteria
\item Those papers marked as relevant by both reviewers were kept and those marked irrelevant by both were thrown out.
\item Papers marked as relevant, or irrelevant by a single reviewer were combined with all papers marked as uncertain and discussed by both reviewers.  From this discussion, papers were either thrown out or kept until the next round of the review.
\end{itemize}

\subsubsection{Step 2: Title Elimination}

The first round of elimination was performed by strict examination based on the title.  Each author was tasked with deciding on elimination by answering the following questions:

\begin{itemize}
\setlength{\itemsep}{0.25pt}
\item Did the title contain a reference to 'role based access control' or 'RBAC'? 
\item Did the title contain a reference to 'model'?
\end{itemize}

\subsubsection{Step 3: Abstract Elimination}

The second round of elimination was based on strict reading of the abstracts of papers that survived title elimination.  Researchers read each abstract and evaluated relevancy based off:

\begin{itemize}
\setlength{\itemsep}{0.25pt}
\item Does the abstract mention a proposed model?
\item Does the abstract mention extension of role-based access control?
\item Does the abstract mention either an implementation, evaluation or domain for their model?
\end{itemize}

\subsubsection{Step 4: Content Elimination}

The final elimination round involved taking the entire paper into consideration and answering five questions that would serve as the basis for elimination.  The data collected by answering these questions served as the basis towards answering the research questions.  The questions each reviewer attempted to answer based on the content of the paper was:

\begin{enumerate}
\setlength{\itemsep}{0.25pt}
\item Does this model extend the core model? (Exclusion)
\item Do the researchers give evidence that RBAC needs extension? (Inclusion)
\item Was the paper and subsequent model inspired by a real world example?  (Conditional Inclusion)
\item Did the researchers offer any evaluation of the proposed model? If yes, how did they do one? If no, why? (Conditional Inclusion)
\item Did the authors implement their model? (Inclusion)
\end{enumerate}

Question 1 was a definitive exclusion criteria as any paper that failed in the affirmative was rejected.  Questions 3 and 4 were marked as conditional includes given that they were connected in making a decision.  A paper that met Question 3 but not 4, or met 4 but not 3 would be included since in some cases the real world examples served as research evaluations and without this conditional include the paper list size would be too small to be significant.

\begin{table}
\centering
\begin{tabular}{ c c | c | c | c | }
\cline{3-5}

 & \multicolumn{1}{ c| }{} & \textbf{Title} & \textbf{Abstract} & \textbf{Content} \\ \hline

\multicolumn{1}{ |c| }{Eric}  &  & 305 & 86 & 46 \\ \hline
\multicolumn{1}{ |c| }{Hwang} &  & 176 & 102 & 42 \\ \hline

\multicolumn{1}{ c| }{} & Overlap & 249 & 51 & 24 \\ \cline{2-5}
\multicolumn{1}{ c| }{} & Disjoint & 232 & 137 & 64 \\ \cline{2-5}
\multicolumn{1}{ c| }{} & Rejected & 208 & 116 & 59 \\ \cline{2-5}
\multicolumn{1}{ c| }{} & Retained & 41 & 21 & 5 \\ \cline{2-5}
\multicolumn{1}{ c| }{} & \textbf{Total} & 290 & 72 & 28 \\ \cline{2-5}

\end{tabular}
\caption{Elimination Rounds}
\label{tab:search_results}
\end{table}


\subsection{Extraction}

After selection of primary sources, the next step was to extract data from each paper that pertained to our research questions in order to look for trends.  The first step was to take the individual data generated from the final elimination round and organize this information around the research questions.  During the paper reading round, and resulting data, the fact that the papers were logically falling into a number of categorizations became evident.  Thus, the first step undertaken was to answer the question of what categories exist for the RBAC model extensions and what papers feel into what categories.
