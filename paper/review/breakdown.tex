\pagebreak

\section{General Breakdown}

\begin{itemize}

\item Broken literature references in Table 5
We address this issue.

\item Tables 3,4,5 are not sorted in any order
We manually sort by the order of the paper.

\item Additional figures/visualizations would help
[TBD] Eric and I would discuss this issue because the reviewer did not clearly which kinds of figures/visualizations would be needed.

\item Reference section has incomplete entries
We address this comment by looking at the each reference item and find missing entry. We check each entry with ACM or IEEE Bibtex. Consistent formatting will be done as well.

\item Spelling, grammar and punctuation flaws
We address this by spell check and grammar check (in Latex compiler). In addition, we convert it to MS word for additional spell check and grammar check. After this step, we will conduct manual proof reading.

\item Table 6 should be revised to have better structure
[TBD]  Eric and I would discuss this issue because the reviewer did not clearly mention what's the issue or how to revise Table 6.

\end{itemize}


\section{Background Breakdown}

\begin{itemize}

\item This section should be shortened to at most 1 page.
We address this comment by revising the background section.

\item Definitions of static and dynamic separation of duty are incorrect.
We find reference of static and dynamic separation of duty from NIST RBAC standard. We refer to "The NIST Model for Role-Based Access Control: Towards A Unified Standard."

\end{itemize}

\section{Methodology Breakdown}

\begin{itemize}

\item Calls into question the "systematic" nature of the paper.

We can address this by adjusting our search method to follow an accepted systematic approach as noted in the SLR literature. Notably the Zhang \cite{zhang2011identifying} approach mentioned in \cite{kitchenham2013systematic}. 
In this approach we do a two phase manual + automatic search. 
The manual phase identifies premier journals and conferences specifically related to our research questions and pulls out the "quasi-gold standard" set of papers.
We derive our search term from this set of papers and use them as an oracle for the precision and sensitivity of our automatic search

\item Why the search we chose? Why not others like Springer? Elsevier?

Looking at the recent work by Kitchenham to review SLRs from 2013 \cite{kitchenham2013systematic}, the literature justifies the use of: 

  \begin{itemize}
    \item ACM Digital Library (already included)
    \item IEEE Explore (already included)
  \end{itemize}

Two of the following indexes:
  \begin{itemize}
    \item SCOPUS
    \item EI Compndix
    \item Web of Science
    \item Google Scholar (already included)
  \end{itemize}

Journals/Conferences of Access Control:

  \begin{itemize}
    \item ACM Transactions on Information and System Security (TISSEC)
    \item International Conference on Emerging Security Information, Systems and Technologies, SECURWARE
    \item International Conference on Information Systems Security
    \item International Conference on Network and System Security
    \item ACM Symposium on Access Control Models and Technologies (SACMAT)
    \item ACM Symposium on Information, Computer and Communications Security
    \item Information Security Conference
    \item International Conference on Information Security and Assurance
    \item International Conference on Information Systems Security
    \item International Conference on Information Assurance and Security
    \item International Symposium on Policies for Distributed Systems and Networks
    \item ACM Conference on Data and Application Security and Privacy
  \end{itemize}

\item Where are the search terms?

We will explicitly enumerate and mention the search term.

\item Search process not described

This will be beefed up with added information from the adjusted search strategy derived from the Zhang method.

\item Search terms too generic. Why isn't search focused on research questions?

Zhang \cite{zhang2011identifying} proposes (based on empirical testing) two methods for selecting search terms.

Subjective - researchers derive search terms based on expert knowledge and the results of the "quasi-gold standard (QGS)" paper set or the use of Kitchenhams suggested population, intervention, comparison, outcomes, context, study designs
Objective - researchers use a textual analysis tool aimed at the QGS paper set to derive most commonly used words and word relationships to then build the search term

\item No mention of using different syntax for different search engines based on how different engines parse search strings.

We will note any differences in how the search term is applied for different search engines.

\item Type of search: full-text? title? abstract?

We can address this by only specifically mentioning it for the manual search process since the automatic search will use common search engines.

\item Whyat type of papers did the authors target? research? white paper? journal? conference?

We can address this by only specifically mentioning it for the manual search process since the automatic search will use common search engines.

\item Was search restricted to a publication period?

We can address this by only specifically mentioning it for the manual search process since the automatic search will use common search engines.

\item Reports of reviewers agreement/disagreement too coarse. IRR analyses recommended by Kitchenham.

We'll calculate and use the Cohen Kappa value for this.

\item Reasoning for stopping criteria never explained, especially technical limitations and what they are.

This one may be trickier as a lot of SLRs don't require a stopping criteria due to how the search term is constructed.
This item may take care of itself with the adjustments to the process and if not we'll specify the technical limitations for any search engine as the reason for why we "stopped".

\item Why not use quasi-gold standard for search threshold?

Zhang method will achieve this.

\item No motivation for why "model" is important as a search term.

Choice of subjective or objective will handle this justification.

\item "conditional inclusion criteria" not allowed by SLR process

We can pair down our inclusion/exclusion criteria and move some of the items into the data collection aspect.
I noted that a number of SLRs use only a few inclusion/exclusion criteria.

\item Corpus too small

We can add a threat to validity if the corpus is too small and note that the search process followed a systematic approach.

\item Question about changing the inclusion/exclusion criteria as not being systematic

We can justify this more in the text as Kitchenham even mentions being able to iterate some elements as the search proceeds.

\item No jutification for deviating from initial procedure

Changing our methodology to Zhang should take care of this.

\item "inspired by real world example" - what does this mean?

\item Table 3/4 inconsistent

\item No papers from joournals/conferences related to access control such as SACMAT, missing papers from ACM TISSEC

We can include these journals as part of the manual QGS approach prior to the automatic search.

\item Why the search chain?

Handled by Zhang.

\item Criteria for selecting sources?

Handled by Zhang method manual and automatic engines listed from Kitchenham.

\item Why "model" in the title is important and not something like "policy"?

This can be handled by the derivation of search terms from the subjective/objective method.

\end{itemize}


\section{RQ1 Breakdown}

\begin{itemize}

\item Not clear how the authors established the common terms.
We find the terms by bottom-up approach by looking at the paper and finding common terms that can characterize the paper. 

\item How did the authors systematically tag the included papers in terms of a content analysis?

\item Was this done ad hoc (during paper reading) or as a preparatory step?
We address this issue by describing our approach. We do it by bottom-up search to find terms and construct it. Need to find such a method for classification (I don't think that our approach is ad-hoc if we find it during paper reading).

\item Authors defined the classification criteria by observation during paper reading.
Same as above.

\item The authors defined comparison items by themselves. Are there existing frameworks for comparing access control policies?

In our SLR, we measure not only access control policies, but also measure approaches of papers.
There are some such as "Hu \& Scarfone" metrics to measure access control policies themselves. We may find some papers that describe metrics. However, their metrics are limited to measure a specific property (such as complexity) of a given access control policy. 


\end{itemize}


\section{RQ2 Breakdown}

\begin{itemize}

\item How do the answers to this RQ relate to the SLR?
We address this issue by checking SLR process to understand what kinds of answers would be expected. Then, we adjust our answers to fit to SLR.

\item This section reads like a discussion based on authors intuitions or conjectures
We would move this section to discussion section if this one may not fit well to the RQs for SLR.

\end{itemize}


\section{RQ4 Breakdown}

\begin{itemize}

\item Why do the authors only distinguish between enterprise/prototype implementations?
In the development process, modeling -> research prototype -> implementations in the real world systems.
Our selected papers dealt with modeling of access control policies. Then, we would like to check whether this model is provided with a prototype and implementation.

\item What about distinguishing different "implementation techniques" ?

\item What about other evaluation techniques such as case studies, static and dynamic analysis, formal proofs or controller experiments for example?
We find from bottom-up approach.

\end{itemize}


\section{RQ6 Breakdown}

\begin{itemize}

\item Authors suggestion of "propositional logic" is purely discussion and not backed by any data from the SLR
We find reference paper about this propositional logic, which is used for papers collected for us. 

\end{itemize}


\section{Discussion Breakdown}

\begin{itemize}

\item Why the "Hu \& Scarfone" metrics?
We use this metric to show additional metrics.

\item Did the authors perform a prior comparative analysis of different metrics?
[TBD] 

\end{itemize}
